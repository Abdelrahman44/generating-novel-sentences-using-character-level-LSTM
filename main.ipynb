{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I construct novel sentences using LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/anna.txt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tuple(set(text))\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {char: i for i, char in int2char.items()}\n",
    "encoded_text = np.array([char2int[char] for char in text])  # [95, 6, 13, 95, ...] corresponding to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 14,  3, 30, 12, 18, 65, 38, 76, 57, 57, 57, 22,  3, 30, 30, 37,\n",
       "       38, 58,  3, 44, 13, 41, 13, 18, 24, 38,  3, 65, 18, 38,  3, 41, 41,\n",
       "       38,  3, 41, 13, 64, 18, 16, 38, 18,  0, 18, 65, 37, 38, 32, 66])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):    #n_labels is the number of unique words (vocabulary), arr is the text\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.    #this creates the on-hot encoded vector\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting batches of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(encoded_text, n_seq, n_steps):\n",
    "    batch_size = n_seq * n_steps\n",
    "    n_batches = len(encoded_text) // batch_size\n",
    "    encoded_text = encoded_text[:n_batches*batch_size]          # drop some data to get only full batches\n",
    "    encoded_text = encoded_text.reshape((n_seq, -1))        \n",
    "    \n",
    "    for i in range(0, encoded_text.shape[1], n_steps):  # iterate on the columns to get the batches\n",
    "        x = encoded_text[:, i:i+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, i+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, 0]\n",
    "        yield x, y                              # x, y are generators that you can use next() on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[15 14  3 30 12 18 65 38 76 57]\n",
      " [38  3 44 38 66 46 12 38 21 46]\n",
      " [ 0 13 66 60 57 57 42 25 18 24]\n",
      " [66 38 20 32 65 13 66 21 38 14]\n",
      " [38 13 12 38 13 24 51 38 24 13]\n",
      " [38 29 12 38 59  3 24 57 46 66]\n",
      " [14 18 66 38 68 46 44 18 38 58]\n",
      " [16 38  2 32 12 38 66 46 59 38]\n",
      " [12 38 13 24 66  9 12 60 38  4]\n",
      " [38 24  3 13 20 38 12 46 38 14]]\n",
      "\n",
      "y\n",
      " [[14  3 30 12 18 65 38 76 57 57]\n",
      " [ 3 44 38 66 46 12 38 21 46 13]\n",
      " [13 66 60 57 57 42 25 18 24 51]\n",
      " [38 20 32 65 13 66 21 38 14 13]\n",
      " [13 12 38 13 24 51 38 24 13 65]\n",
      " [29 12 38 59  3 24 57 46 66 41]\n",
      " [18 66 38 68 46 44 18 38 58 46]\n",
      " [38  2 32 12 38 66 46 59 38 24]\n",
      " [38 13 24 66  9 12 60 38  4 14]\n",
      " [24  3 13 20 38 12 46 38 14 18]]\n",
      "(10, 50) (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_text, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, n_steps=100, n_hidden=256, n_layers=4, drop_prop=0.5, lr=0.001):\n",
    "        # n_steps: number of elements in each sequence in each batch\n",
    "        # n_hidden: number of output elements of the intermediate layers\n",
    "        # n_layers: number of LSTM layers to use\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prop = drop_prop\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_steps = n_steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.int2char = dict(enumerate(self.vocab))\n",
    "        self.char2int = {char: i for i, char in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.vocab), n_hidden, n_layers, dropout=drop_prop, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prop)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.vocab))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x.view(x.shape[0]*x.shape[1], self.n_hidden))\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, x, h=None, cuda=False):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "            \n",
    "        x = np.array([[self.char2int[x]]])\n",
    "        x = one_hot_encode(x, len(self.char2int))\n",
    "        x = torch.from_numpy(x)\n",
    "        \n",
    "        if h == None:\n",
    "            h = init_hidden(1)\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            \n",
    "        out, h = self.forward(x, h)\n",
    "        out = F.softmax(out).data\n",
    "        \n",
    "        out = out.numpy().squeeze()\n",
    "        \n",
    "        if cuda:\n",
    "            out.cpu()\n",
    "        \n",
    "        return self.int2char[np.argmax(out)]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "    \n",
    "net = CharRNN(vocab, n_hidden=512)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, encoded_text, epochs=10, n_seq=10, n_steps=50, lr=0.001, cuda=True, clip=5, print_every=10):\n",
    "    \n",
    "    net.train(True)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_txt, val_txt = encoded_text[:-int(0.2*len(encoded_text))], encoded_text[-int(0.2*len(encoded_text)):]\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(net.vocab)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seq)\n",
    "        for x, y in get_batches(encoded_text, n_seq, n_steps):\n",
    "            counter +=1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seq*n_steps))\n",
    "            loss.backward()\n",
    "            \n",
    "                   # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seq)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_txt, n_seq, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seq*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.3681... Val Loss: 3.3758\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2802... Val Loss: 3.2862\n",
      "Epoch: 1/10... Step: 30... Loss: 3.2550... Val Loss: 3.2658\n",
      "Epoch: 1/10... Step: 40... Loss: 3.2312... Val Loss: 3.2467\n",
      "Epoch: 1/10... Step: 50... Loss: 3.2093... Val Loss: 3.2275\n",
      "Epoch: 1/10... Step: 60... Loss: 3.1494... Val Loss: 3.1705\n",
      "Epoch: 1/10... Step: 70... Loss: 3.0485... Val Loss: 3.0506\n",
      "Epoch: 1/10... Step: 80... Loss: 2.9688... Val Loss: 2.9597\n",
      "Epoch: 1/10... Step: 90... Loss: 2.9170... Val Loss: 2.8981\n",
      "Epoch: 1/10... Step: 100... Loss: 2.8296... Val Loss: 2.8334\n",
      "Epoch: 1/10... Step: 110... Loss: 2.7464... Val Loss: 2.7424\n",
      "Epoch: 1/10... Step: 120... Loss: 2.6562... Val Loss: 2.6452\n",
      "Epoch: 1/10... Step: 130... Loss: 2.5755... Val Loss: 2.5752\n",
      "Epoch: 1/10... Step: 140... Loss: 2.5268... Val Loss: 2.5232\n",
      "Epoch: 1/10... Step: 150... Loss: 2.4682... Val Loss: 2.4816\n",
      "Epoch: 2/10... Step: 160... Loss: 2.4284... Val Loss: 2.4525\n",
      "Epoch: 2/10... Step: 170... Loss: 2.3771... Val Loss: 2.4151\n",
      "Epoch: 2/10... Step: 180... Loss: 2.3589... Val Loss: 2.3759\n",
      "Epoch: 2/10... Step: 190... Loss: 2.3172... Val Loss: 2.3445\n",
      "Epoch: 2/10... Step: 200... Loss: 2.2844... Val Loss: 2.3128\n",
      "Epoch: 2/10... Step: 210... Loss: 2.2462... Val Loss: 2.2858\n",
      "Epoch: 2/10... Step: 220... Loss: 2.2142... Val Loss: 2.2502\n",
      "Epoch: 2/10... Step: 230... Loss: 2.1866... Val Loss: 2.2179\n",
      "Epoch: 2/10... Step: 240... Loss: 2.1546... Val Loss: 2.1849\n",
      "Epoch: 2/10... Step: 250... Loss: 2.1386... Val Loss: 2.1581\n",
      "Epoch: 2/10... Step: 260... Loss: 2.0888... Val Loss: 2.1276\n",
      "Epoch: 2/10... Step: 270... Loss: 2.0830... Val Loss: 2.0978\n",
      "Epoch: 2/10... Step: 280... Loss: 2.0476... Val Loss: 2.0739\n",
      "Epoch: 2/10... Step: 290... Loss: 2.0430... Val Loss: 2.0487\n",
      "Epoch: 2/10... Step: 300... Loss: 2.0119... Val Loss: 2.0274\n",
      "Epoch: 2/10... Step: 310... Loss: 1.9979... Val Loss: 2.0063\n",
      "Epoch: 3/10... Step: 320... Loss: 1.9418... Val Loss: 1.9877\n",
      "Epoch: 3/10... Step: 330... Loss: 1.9074... Val Loss: 1.9638\n",
      "Epoch: 3/10... Step: 340... Loss: 1.9297... Val Loss: 1.9444\n",
      "Epoch: 3/10... Step: 350... Loss: 1.8931... Val Loss: 1.9268\n",
      "Epoch: 3/10... Step: 360... Loss: 1.8921... Val Loss: 1.9089\n",
      "Epoch: 3/10... Step: 370... Loss: 1.8439... Val Loss: 1.8902\n",
      "Epoch: 3/10... Step: 380... Loss: 1.8726... Val Loss: 1.8768\n",
      "Epoch: 3/10... Step: 390... Loss: 1.8350... Val Loss: 1.8562\n",
      "Epoch: 3/10... Step: 400... Loss: 1.8140... Val Loss: 1.8432\n",
      "Epoch: 3/10... Step: 410... Loss: 1.8188... Val Loss: 1.8274\n",
      "Epoch: 3/10... Step: 420... Loss: 1.8125... Val Loss: 1.8066\n",
      "Epoch: 3/10... Step: 430... Loss: 1.7801... Val Loss: 1.7919\n",
      "Epoch: 3/10... Step: 440... Loss: 1.7540... Val Loss: 1.7762\n",
      "Epoch: 3/10... Step: 450... Loss: 1.7598... Val Loss: 1.7632\n",
      "Epoch: 3/10... Step: 460... Loss: 1.7580... Val Loss: 1.7519\n",
      "Epoch: 4/10... Step: 470... Loss: 1.6886... Val Loss: 1.7373\n",
      "Epoch: 4/10... Step: 480... Loss: 1.6655... Val Loss: 1.7258\n",
      "Epoch: 4/10... Step: 490... Loss: 1.6750... Val Loss: 1.7102\n",
      "Epoch: 4/10... Step: 500... Loss: 1.6785... Val Loss: 1.6994\n",
      "Epoch: 4/10... Step: 510... Loss: 1.6669... Val Loss: 1.6895\n",
      "Epoch: 4/10... Step: 520... Loss: 1.6401... Val Loss: 1.6770\n",
      "Epoch: 4/10... Step: 530... Loss: 1.6500... Val Loss: 1.6666\n",
      "Epoch: 4/10... Step: 540... Loss: 1.6539... Val Loss: 1.6595\n",
      "Epoch: 4/10... Step: 550... Loss: 1.6254... Val Loss: 1.6447\n",
      "Epoch: 4/10... Step: 560... Loss: 1.6100... Val Loss: 1.6387\n",
      "Epoch: 4/10... Step: 570... Loss: 1.5714... Val Loss: 1.6267\n",
      "Epoch: 4/10... Step: 580... Loss: 1.5677... Val Loss: 1.6154\n",
      "Epoch: 4/10... Step: 590... Loss: 1.5705... Val Loss: 1.6076\n",
      "Epoch: 4/10... Step: 600... Loss: 1.5890... Val Loss: 1.5990\n",
      "Epoch: 4/10... Step: 610... Loss: 1.5813... Val Loss: 1.5891\n",
      "Epoch: 4/10... Step: 620... Loss: 1.5747... Val Loss: 1.5834\n",
      "Epoch: 5/10... Step: 630... Loss: 1.5527... Val Loss: 1.5756\n",
      "Epoch: 5/10... Step: 640... Loss: 1.5476... Val Loss: 1.5704\n",
      "Epoch: 5/10... Step: 650... Loss: 1.5638... Val Loss: 1.5583\n",
      "Epoch: 5/10... Step: 660... Loss: 1.5362... Val Loss: 1.5513\n",
      "Epoch: 5/10... Step: 670... Loss: 1.5260... Val Loss: 1.5436\n",
      "Epoch: 5/10... Step: 680... Loss: 1.4893... Val Loss: 1.5376\n",
      "Epoch: 5/10... Step: 690... Loss: 1.5402... Val Loss: 1.5330\n",
      "Epoch: 5/10... Step: 700... Loss: 1.4956... Val Loss: 1.5237\n",
      "Epoch: 5/10... Step: 710... Loss: 1.4936... Val Loss: 1.5254\n",
      "Epoch: 5/10... Step: 720... Loss: 1.5161... Val Loss: 1.5172\n",
      "Epoch: 5/10... Step: 730... Loss: 1.5171... Val Loss: 1.5071\n",
      "Epoch: 5/10... Step: 740... Loss: 1.4886... Val Loss: 1.4988\n",
      "Epoch: 5/10... Step: 750... Loss: 1.4747... Val Loss: 1.4958\n",
      "Epoch: 5/10... Step: 760... Loss: 1.4925... Val Loss: 1.4936\n",
      "Epoch: 5/10... Step: 770... Loss: 1.4781... Val Loss: 1.4891\n",
      "Epoch: 6/10... Step: 780... Loss: 1.4388... Val Loss: 1.4835\n",
      "Epoch: 6/10... Step: 790... Loss: 1.4102... Val Loss: 1.4755\n",
      "Epoch: 6/10... Step: 800... Loss: 1.4455... Val Loss: 1.4725\n",
      "Epoch: 6/10... Step: 810... Loss: 1.4519... Val Loss: 1.4674\n",
      "Epoch: 6/10... Step: 820... Loss: 1.4349... Val Loss: 1.4641\n",
      "Epoch: 6/10... Step: 830... Loss: 1.4295... Val Loss: 1.4579\n",
      "Epoch: 6/10... Step: 840... Loss: 1.4445... Val Loss: 1.4520\n",
      "Epoch: 6/10... Step: 850... Loss: 1.4508... Val Loss: 1.4497\n",
      "Epoch: 6/10... Step: 860... Loss: 1.4259... Val Loss: 1.4450\n",
      "Epoch: 6/10... Step: 870... Loss: 1.4300... Val Loss: 1.4435\n",
      "Epoch: 6/10... Step: 880... Loss: 1.4007... Val Loss: 1.4356\n",
      "Epoch: 6/10... Step: 890... Loss: 1.3916... Val Loss: 1.4326\n",
      "Epoch: 6/10... Step: 900... Loss: 1.3971... Val Loss: 1.4309\n",
      "Epoch: 6/10... Step: 910... Loss: 1.4120... Val Loss: 1.4332\n",
      "Epoch: 6/10... Step: 920... Loss: 1.4382... Val Loss: 1.4260\n",
      "Epoch: 6/10... Step: 930... Loss: 1.4292... Val Loss: 1.4224\n",
      "Epoch: 7/10... Step: 940... Loss: 1.3914... Val Loss: 1.4167\n",
      "Epoch: 7/10... Step: 950... Loss: 1.3970... Val Loss: 1.4166\n",
      "Epoch: 7/10... Step: 960... Loss: 1.4074... Val Loss: 1.4139\n",
      "Epoch: 7/10... Step: 970... Loss: 1.3899... Val Loss: 1.4133\n",
      "Epoch: 7/10... Step: 980... Loss: 1.3910... Val Loss: 1.4051\n",
      "Epoch: 7/10... Step: 990... Loss: 1.3608... Val Loss: 1.4013\n",
      "Epoch: 7/10... Step: 1000... Loss: 1.3962... Val Loss: 1.3990\n",
      "Epoch: 7/10... Step: 1010... Loss: 1.3629... Val Loss: 1.3955\n",
      "Epoch: 7/10... Step: 1020... Loss: 1.3732... Val Loss: 1.3932\n",
      "Epoch: 7/10... Step: 1030... Loss: 1.4042... Val Loss: 1.3898\n",
      "Epoch: 7/10... Step: 1040... Loss: 1.3942... Val Loss: 1.3897\n",
      "Epoch: 7/10... Step: 1050... Loss: 1.3759... Val Loss: 1.3835\n",
      "Epoch: 7/10... Step: 1060... Loss: 1.3566... Val Loss: 1.3818\n",
      "Epoch: 7/10... Step: 1070... Loss: 1.3877... Val Loss: 1.3807\n",
      "Epoch: 7/10... Step: 1080... Loss: 1.3565... Val Loss: 1.3760\n",
      "Epoch: 8/10... Step: 1090... Loss: 1.3281... Val Loss: 1.3757\n",
      "Epoch: 8/10... Step: 1100... Loss: 1.3089... Val Loss: 1.3718\n",
      "Epoch: 8/10... Step: 1110... Loss: 1.3468... Val Loss: 1.3708\n",
      "Epoch: 8/10... Step: 1120... Loss: 1.3579... Val Loss: 1.3650\n",
      "Epoch: 8/10... Step: 1130... Loss: 1.3358... Val Loss: 1.3648\n",
      "Epoch: 8/10... Step: 1140... Loss: 1.3388... Val Loss: 1.3623\n",
      "Epoch: 8/10... Step: 1150... Loss: 1.3529... Val Loss: 1.3608\n",
      "Epoch: 8/10... Step: 1160... Loss: 1.3721... Val Loss: 1.3606\n",
      "Epoch: 8/10... Step: 1170... Loss: 1.3536... Val Loss: 1.3547\n",
      "Epoch: 8/10... Step: 1180... Loss: 1.3317... Val Loss: 1.3563\n",
      "Epoch: 8/10... Step: 1190... Loss: 1.3196... Val Loss: 1.3551\n",
      "Epoch: 8/10... Step: 1200... Loss: 1.3110... Val Loss: 1.3527\n",
      "Epoch: 8/10... Step: 1210... Loss: 1.3085... Val Loss: 1.3467\n",
      "Epoch: 8/10... Step: 1220... Loss: 1.3219... Val Loss: 1.3443\n",
      "Epoch: 8/10... Step: 1230... Loss: 1.3532... Val Loss: 1.3422\n",
      "Epoch: 8/10... Step: 1240... Loss: 1.3514... Val Loss: 1.3429\n",
      "Epoch: 9/10... Step: 1250... Loss: 1.3163... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 1260... Loss: 1.3227... Val Loss: 1.3384\n",
      "Epoch: 9/10... Step: 1270... Loss: 1.3326... Val Loss: 1.3349\n",
      "Epoch: 9/10... Step: 1280... Loss: 1.3161... Val Loss: 1.3337\n",
      "Epoch: 9/10... Step: 1290... Loss: 1.3190... Val Loss: 1.3323\n",
      "Epoch: 9/10... Step: 1300... Loss: 1.2816... Val Loss: 1.3301\n",
      "Epoch: 9/10... Step: 1310... Loss: 1.3188... Val Loss: 1.3257\n",
      "Epoch: 9/10... Step: 1320... Loss: 1.2938... Val Loss: 1.3276\n",
      "Epoch: 9/10... Step: 1330... Loss: 1.3080... Val Loss: 1.3245\n",
      "Epoch: 9/10... Step: 1340... Loss: 1.3309... Val Loss: 1.3235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 1350... Loss: 1.3277... Val Loss: 1.3211\n",
      "Epoch: 9/10... Step: 1360... Loss: 1.3016... Val Loss: 1.3200\n",
      "Epoch: 9/10... Step: 1370... Loss: 1.2869... Val Loss: 1.3185\n",
      "Epoch: 9/10... Step: 1380... Loss: 1.3175... Val Loss: 1.3164\n",
      "Epoch: 9/10... Step: 1390... Loss: 1.3051... Val Loss: 1.3168\n",
      "Epoch: 10/10... Step: 1400... Loss: 1.2517... Val Loss: 1.3133\n",
      "Epoch: 10/10... Step: 1410... Loss: 1.2434... Val Loss: 1.3107\n",
      "Epoch: 10/10... Step: 1420... Loss: 1.2910... Val Loss: 1.3107\n",
      "Epoch: 10/10... Step: 1430... Loss: 1.2985... Val Loss: 1.3112\n",
      "Epoch: 10/10... Step: 1440... Loss: 1.2687... Val Loss: 1.3094\n",
      "Epoch: 10/10... Step: 1450... Loss: 1.2806... Val Loss: 1.3056\n",
      "Epoch: 10/10... Step: 1460... Loss: 1.2990... Val Loss: 1.3046\n",
      "Epoch: 10/10... Step: 1470... Loss: 1.3028... Val Loss: 1.3032\n",
      "Epoch: 10/10... Step: 1480... Loss: 1.2938... Val Loss: 1.3012\n",
      "Epoch: 10/10... Step: 1490... Loss: 1.2919... Val Loss: 1.3023\n",
      "Epoch: 10/10... Step: 1500... Loss: 1.2578... Val Loss: 1.2998\n",
      "Epoch: 10/10... Step: 1510... Loss: 1.2490... Val Loss: 1.2978\n",
      "Epoch: 10/10... Step: 1520... Loss: 1.2626... Val Loss: 1.2968\n",
      "Epoch: 10/10... Step: 1530... Loss: 1.2723... Val Loss: 1.2949\n",
      "Epoch: 10/10... Step: 1540... Loss: 1.3042... Val Loss: 1.2932\n",
      "Epoch: 10/10... Step: 1550... Loss: 1.3071... Val Loss: 1.2927\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded_text, epochs=10, n_seq=128, n_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '4_LSTMs.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.vocab}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference / generating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(net, length, first_letters, cuda=True):\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "        \n",
    "    net.eval()\n",
    "    \n",
    "    chars = [char for char in first_letters]\n",
    "    h = net.init_hidden(1)\n",
    "    for i in range(length):\n",
    "        char = net.predict(chars[-1], h=h, cuda=cuda)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ingramai/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-11324e24f7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-42edb2bea41b>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(net, length, first_letters, cuda)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c6819d58301d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, h, cuda)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "generated = generate(net, 50, \"I lov\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (cv-nd)",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
