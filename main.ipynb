{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I construct novel sentences using LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/anna.txt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tuple(set(text))\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {char: i for i, char in int2char.items()}\n",
    "encoded_text = np.array([char2int[char] for char in text])  # [95, 6, 13, 95, ...] corresponding to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 14,  3, 30, 12, 18, 65, 38, 76, 57, 57, 57, 22,  3, 30, 30, 37,\n",
       "       38, 58,  3, 44, 13, 41, 13, 18, 24, 38,  3, 65, 18, 38,  3, 41, 41,\n",
       "       38,  3, 41, 13, 64, 18, 16, 38, 18,  0, 18, 65, 37, 38, 32, 66])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):    #n_labels is the number of unique words (vocabulary), arr is the text\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.    #this creates the on-hot encoded vector\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting batches of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(encoded_text, n_seq, n_steps):\n",
    "    batch_size = n_seq * n_steps\n",
    "    n_batches = len(encoded_text) // batch_size\n",
    "    encoded_text = encoded_text[:n_batches*batch_size]          # drop some data to get only full batches\n",
    "    encoded_text = encoded_text.reshape((n_seq, -1))        \n",
    "    \n",
    "    for i in range(0, encoded_text.shape[1], n_steps):  # iterate on the columns to get the batches\n",
    "        x = encoded_text[:, i:i+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, i+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, 0]\n",
    "        yield x, y                              # x, y are generators that you can use next() on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[15 14  3 30 12 18 65 38 76 57]\n",
      " [38  3 44 38 66 46 12 38 21 46]\n",
      " [ 0 13 66 60 57 57 42 25 18 24]\n",
      " [66 38 20 32 65 13 66 21 38 14]\n",
      " [38 13 12 38 13 24 51 38 24 13]\n",
      " [38 29 12 38 59  3 24 57 46 66]\n",
      " [14 18 66 38 68 46 44 18 38 58]\n",
      " [16 38  2 32 12 38 66 46 59 38]\n",
      " [12 38 13 24 66  9 12 60 38  4]\n",
      " [38 24  3 13 20 38 12 46 38 14]]\n",
      "\n",
      "y\n",
      " [[14  3 30 12 18 65 38 76 57 57]\n",
      " [ 3 44 38 66 46 12 38 21 46 13]\n",
      " [13 66 60 57 57 42 25 18 24 51]\n",
      " [38 20 32 65 13 66 21 38 14 13]\n",
      " [13 12 38 13 24 51 38 24 13 65]\n",
      " [29 12 38 59  3 24 57 46 66 41]\n",
      " [18 66 38 68 46 44 18 38 58 46]\n",
      " [38  2 32 12 38 66 46 59 38 24]\n",
      " [38 13 24 66  9 12 60 38  4 14]\n",
      " [24  3 13 20 38 12 46 38 14 18]]\n",
      "(10, 50) (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_text, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, n_steps=100, n_hidden=256, n_layers=2, drop_prop=0.5, lr=0.001):\n",
    "        # n_steps: number of elements in each sequence in each batch\n",
    "        # n_hidden: number of output elements of the intermediate layers\n",
    "        # n_layers: number of LSTM layers to use\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prop = drop_prop\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_steps = n_steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.int2char = dict(enumerate(self.vocab))\n",
    "        self.char2int = {char: i for i, char in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.vocab), n_hidden, n_layers, dropout=drop_prop, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prop)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.vocab))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x.view(x.shape[0]*x.shape[1], self.n_hidden))\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, x, h=None, cuda=False):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "            \n",
    "        x = np.array([[self.char2int[x]]])\n",
    "        x = one_hot_encode(x, len(self.char2int))\n",
    "        x = torch.from_numpy(x)\n",
    "        \n",
    "        if h == None:\n",
    "            h = init_hidden(1)\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            \n",
    "        out, h = self.forward(x, h)\n",
    "        out = F.softmax(out)\n",
    "        \n",
    "        out = out.numpy().squeeze()\n",
    "        \n",
    "        return self.int2char[np.argmax(out)]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if 'net' in locals():\n",
    "    del net\n",
    "    \n",
    "net = CharRNN(vocab, n_hidden=512)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, encoded_text, epochs=10, n_seq=10, n_steps=50, lr=0.001, cuda=True, clip=5, print_every=10):\n",
    "    \n",
    "    net.train(True)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_txt, val_txt = encoded_text[:-int(0.2*len(encoded_text))], encoded_text[-int(0.2*len(encoded_text)):]\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(net.vocab)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seq)\n",
    "        for x, y in get_batches(encoded_text, n_seq, n_steps):\n",
    "            counter +=1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seq*n_steps))\n",
    "            loss.backward()\n",
    "            \n",
    "                   # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seq)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_txt, n_seq, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seq*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 2.1929... Val Loss: 2.2415\n",
      "Epoch: 1/10... Step: 20... Loss: 2.1519... Val Loss: 2.2085\n",
      "Epoch: 1/10... Step: 30... Loss: 2.1688... Val Loss: 2.1768\n",
      "Epoch: 1/10... Step: 40... Loss: 2.1106... Val Loss: 2.1537\n",
      "Epoch: 1/10... Step: 50... Loss: 2.1321... Val Loss: 2.1322\n",
      "Epoch: 1/10... Step: 60... Loss: 2.0721... Val Loss: 2.1138\n",
      "Epoch: 1/10... Step: 70... Loss: 2.1030... Val Loss: 2.0917\n",
      "Epoch: 1/10... Step: 80... Loss: 2.0450... Val Loss: 2.0765\n",
      "Epoch: 1/10... Step: 90... Loss: 2.0543... Val Loss: 2.0521\n",
      "Epoch: 1/10... Step: 100... Loss: 2.0222... Val Loss: 2.0376\n",
      "Epoch: 1/10... Step: 110... Loss: 2.0035... Val Loss: 2.0175\n",
      "Epoch: 1/10... Step: 120... Loss: 1.9916... Val Loss: 2.0024\n",
      "Epoch: 1/10... Step: 130... Loss: 1.9709... Val Loss: 1.9856\n",
      "Epoch: 1/10... Step: 140... Loss: 1.9742... Val Loss: 1.9712\n",
      "Epoch: 1/10... Step: 150... Loss: 1.9517... Val Loss: 1.9558\n",
      "Epoch: 2/10... Step: 160... Loss: 1.8982... Val Loss: 1.9458\n",
      "Epoch: 2/10... Step: 170... Loss: 1.8800... Val Loss: 1.9295\n",
      "Epoch: 2/10... Step: 180... Loss: 1.8863... Val Loss: 1.9186\n",
      "Epoch: 2/10... Step: 190... Loss: 1.8813... Val Loss: 1.9015\n",
      "Epoch: 2/10... Step: 200... Loss: 1.8679... Val Loss: 1.8899\n",
      "Epoch: 2/10... Step: 210... Loss: 1.8469... Val Loss: 1.8777\n",
      "Epoch: 2/10... Step: 220... Loss: 1.8447... Val Loss: 1.8763\n",
      "Epoch: 2/10... Step: 230... Loss: 1.8401... Val Loss: 1.8606\n",
      "Epoch: 2/10... Step: 240... Loss: 1.8079... Val Loss: 1.8448\n",
      "Epoch: 2/10... Step: 250... Loss: 1.8085... Val Loss: 1.8348\n",
      "Epoch: 2/10... Step: 260... Loss: 1.7819... Val Loss: 1.8255\n",
      "Epoch: 2/10... Step: 270... Loss: 1.7726... Val Loss: 1.8125\n",
      "Epoch: 2/10... Step: 280... Loss: 1.7743... Val Loss: 1.8000\n",
      "Epoch: 2/10... Step: 290... Loss: 1.7755... Val Loss: 1.7896\n",
      "Epoch: 2/10... Step: 300... Loss: 1.7815... Val Loss: 1.7829\n",
      "Epoch: 2/10... Step: 310... Loss: 1.7639... Val Loss: 1.7736\n",
      "Epoch: 3/10... Step: 320... Loss: 1.7415... Val Loss: 1.7647\n",
      "Epoch: 3/10... Step: 330... Loss: 1.7155... Val Loss: 1.7557\n",
      "Epoch: 3/10... Step: 340... Loss: 1.7407... Val Loss: 1.7436\n",
      "Epoch: 3/10... Step: 350... Loss: 1.7153... Val Loss: 1.7357\n",
      "Epoch: 3/10... Step: 360... Loss: 1.7162... Val Loss: 1.7279\n",
      "Epoch: 3/10... Step: 370... Loss: 1.6715... Val Loss: 1.7214\n",
      "Epoch: 3/10... Step: 380... Loss: 1.7159... Val Loss: 1.7329\n",
      "Epoch: 3/10... Step: 390... Loss: 1.6822... Val Loss: 1.7113\n",
      "Epoch: 3/10... Step: 400... Loss: 1.6778... Val Loss: 1.6997\n",
      "Epoch: 3/10... Step: 410... Loss: 1.6805... Val Loss: 1.6903\n",
      "Epoch: 3/10... Step: 420... Loss: 1.6784... Val Loss: 1.6826\n",
      "Epoch: 3/10... Step: 430... Loss: 1.6501... Val Loss: 1.6747\n",
      "Epoch: 3/10... Step: 440... Loss: 1.6374... Val Loss: 1.6670\n",
      "Epoch: 3/10... Step: 450... Loss: 1.6593... Val Loss: 1.6604\n",
      "Epoch: 3/10... Step: 460... Loss: 1.6595... Val Loss: 1.6539\n",
      "Epoch: 4/10... Step: 470... Loss: 1.6122... Val Loss: 1.6510\n",
      "Epoch: 4/10... Step: 480... Loss: 1.5877... Val Loss: 1.6434\n",
      "Epoch: 4/10... Step: 490... Loss: 1.6051... Val Loss: 1.6369\n",
      "Epoch: 4/10... Step: 500... Loss: 1.6012... Val Loss: 1.6272\n",
      "Epoch: 4/10... Step: 510... Loss: 1.5965... Val Loss: 1.6186\n",
      "Epoch: 4/10... Step: 520... Loss: 1.5851... Val Loss: 1.6137\n",
      "Epoch: 4/10... Step: 530... Loss: 1.5902... Val Loss: 1.6111\n",
      "Epoch: 4/10... Step: 540... Loss: 1.6004... Val Loss: 1.6067\n",
      "Epoch: 4/10... Step: 550... Loss: 1.5858... Val Loss: 1.6113\n",
      "Epoch: 4/10... Step: 560... Loss: 1.5707... Val Loss: 1.5971\n",
      "Epoch: 4/10... Step: 570... Loss: 1.5484... Val Loss: 1.5906\n",
      "Epoch: 4/10... Step: 580... Loss: 1.5371... Val Loss: 1.5813\n",
      "Epoch: 4/10... Step: 590... Loss: 1.5415... Val Loss: 1.5783\n",
      "Epoch: 4/10... Step: 600... Loss: 1.5644... Val Loss: 1.5727\n",
      "Epoch: 4/10... Step: 610... Loss: 1.5720... Val Loss: 1.5663\n",
      "Epoch: 4/10... Step: 620... Loss: 1.5566... Val Loss: 1.5593\n",
      "Epoch: 5/10... Step: 630... Loss: 1.5326... Val Loss: 1.5591\n",
      "Epoch: 5/10... Step: 640... Loss: 1.5372... Val Loss: 1.5598\n",
      "Epoch: 5/10... Step: 650... Loss: 1.5480... Val Loss: 1.5530\n",
      "Epoch: 5/10... Step: 660... Loss: 1.5356... Val Loss: 1.5463\n",
      "Epoch: 5/10... Step: 670... Loss: 1.5319... Val Loss: 1.5432\n",
      "Epoch: 5/10... Step: 680... Loss: 1.4881... Val Loss: 1.5430\n",
      "Epoch: 5/10... Step: 690... Loss: 1.5435... Val Loss: 1.5349\n",
      "Epoch: 5/10... Step: 700... Loss: 1.4854... Val Loss: 1.5264\n",
      "Epoch: 5/10... Step: 710... Loss: 1.5125... Val Loss: 1.5232\n",
      "Epoch: 5/10... Step: 720... Loss: 1.5317... Val Loss: 1.5182\n",
      "Epoch: 5/10... Step: 730... Loss: 1.5241... Val Loss: 1.5148\n",
      "Epoch: 5/10... Step: 740... Loss: 1.4925... Val Loss: 1.5070\n",
      "Epoch: 5/10... Step: 750... Loss: 1.4877... Val Loss: 1.5174\n",
      "Epoch: 5/10... Step: 760... Loss: 1.5078... Val Loss: 1.5050\n",
      "Epoch: 5/10... Step: 770... Loss: 1.5007... Val Loss: 1.4993\n",
      "Epoch: 6/10... Step: 780... Loss: 1.4553... Val Loss: 1.4953\n",
      "Epoch: 6/10... Step: 790... Loss: 1.4258... Val Loss: 1.4969\n",
      "Epoch: 6/10... Step: 800... Loss: 1.4639... Val Loss: 1.4926\n",
      "Epoch: 6/10... Step: 810... Loss: 1.4733... Val Loss: 1.4836\n",
      "Epoch: 6/10... Step: 820... Loss: 1.4568... Val Loss: 1.4800\n",
      "Epoch: 6/10... Step: 830... Loss: 1.4471... Val Loss: 1.4755\n",
      "Epoch: 6/10... Step: 840... Loss: 1.4670... Val Loss: 1.4734\n",
      "Epoch: 6/10... Step: 850... Loss: 1.4775... Val Loss: 1.4717\n",
      "Epoch: 6/10... Step: 860... Loss: 1.4625... Val Loss: 1.4716\n",
      "Epoch: 6/10... Step: 870... Loss: 1.4472... Val Loss: 1.4734\n",
      "Epoch: 6/10... Step: 880... Loss: 1.4243... Val Loss: 1.4625\n",
      "Epoch: 6/10... Step: 890... Loss: 1.4149... Val Loss: 1.4569\n",
      "Epoch: 6/10... Step: 900... Loss: 1.4211... Val Loss: 1.4596\n",
      "Epoch: 6/10... Step: 910... Loss: 1.4306... Val Loss: 1.4521\n",
      "Epoch: 6/10... Step: 920... Loss: 1.4576... Val Loss: 1.4459\n",
      "Epoch: 6/10... Step: 930... Loss: 1.4558... Val Loss: 1.4480\n",
      "Epoch: 7/10... Step: 940... Loss: 1.4419... Val Loss: 1.4454\n",
      "Epoch: 7/10... Step: 950... Loss: 1.4331... Val Loss: 1.4401\n",
      "Epoch: 7/10... Step: 960... Loss: 1.4375... Val Loss: 1.4382\n",
      "Epoch: 7/10... Step: 970... Loss: 1.4162... Val Loss: 1.4341\n",
      "Epoch: 7/10... Step: 980... Loss: 1.4267... Val Loss: 1.4354\n",
      "Epoch: 7/10... Step: 990... Loss: 1.3890... Val Loss: 1.4361\n",
      "Epoch: 7/10... Step: 1000... Loss: 1.4273... Val Loss: 1.4292\n",
      "Epoch: 7/10... Step: 1010... Loss: 1.3917... Val Loss: 1.4283\n",
      "Epoch: 7/10... Step: 1020... Loss: 1.4174... Val Loss: 1.4306\n",
      "Epoch: 7/10... Step: 1030... Loss: 1.4320... Val Loss: 1.4242\n",
      "Epoch: 7/10... Step: 1040... Loss: 1.4269... Val Loss: 1.4207\n",
      "Epoch: 7/10... Step: 1050... Loss: 1.3995... Val Loss: 1.4161\n",
      "Epoch: 7/10... Step: 1060... Loss: 1.3921... Val Loss: 1.4198\n",
      "Epoch: 7/10... Step: 1070... Loss: 1.4181... Val Loss: 1.4127\n",
      "Epoch: 7/10... Step: 1080... Loss: 1.4077... Val Loss: 1.4083\n",
      "Epoch: 8/10... Step: 1090... Loss: 1.3588... Val Loss: 1.4033\n",
      "Epoch: 8/10... Step: 1100... Loss: 1.3442... Val Loss: 1.4000\n",
      "Epoch: 8/10... Step: 1110... Loss: 1.3801... Val Loss: 1.4022\n",
      "Epoch: 8/10... Step: 1120... Loss: 1.3824... Val Loss: 1.3983\n",
      "Epoch: 8/10... Step: 1130... Loss: 1.3786... Val Loss: 1.3968\n",
      "Epoch: 8/10... Step: 1140... Loss: 1.3637... Val Loss: 1.3980\n",
      "Epoch: 8/10... Step: 1150... Loss: 1.3793... Val Loss: 1.3925\n",
      "Epoch: 8/10... Step: 1160... Loss: 1.3977... Val Loss: 1.3935\n",
      "Epoch: 8/10... Step: 1170... Loss: 1.3810... Val Loss: 1.3908\n",
      "Epoch: 8/10... Step: 1180... Loss: 1.3727... Val Loss: 1.3875\n",
      "Epoch: 8/10... Step: 1190... Loss: 1.3438... Val Loss: 1.3837\n",
      "Epoch: 8/10... Step: 1200... Loss: 1.3372... Val Loss: 1.3827\n",
      "Epoch: 8/10... Step: 1210... Loss: 1.3365... Val Loss: 1.3783\n",
      "Epoch: 8/10... Step: 1220... Loss: 1.3608... Val Loss: 1.3815\n",
      "Epoch: 8/10... Step: 1230... Loss: 1.3875... Val Loss: 1.3772\n",
      "Epoch: 8/10... Step: 1240... Loss: 1.3818... Val Loss: 1.3758\n",
      "Epoch: 9/10... Step: 1250... Loss: 1.3545... Val Loss: 1.3691\n",
      "Epoch: 9/10... Step: 1260... Loss: 1.3638... Val Loss: 1.3784\n",
      "Epoch: 9/10... Step: 1270... Loss: 1.3828... Val Loss: 1.3718\n",
      "Epoch: 9/10... Step: 1280... Loss: 1.3498... Val Loss: 1.3689\n",
      "Epoch: 9/10... Step: 1290... Loss: 1.3528... Val Loss: 1.3641\n",
      "Epoch: 9/10... Step: 1300... Loss: 1.3295... Val Loss: 1.3701\n",
      "Epoch: 9/10... Step: 1310... Loss: 1.3694... Val Loss: 1.3646\n",
      "Epoch: 9/10... Step: 1320... Loss: 1.3289... Val Loss: 1.3615\n",
      "Epoch: 9/10... Step: 1330... Loss: 1.3483... Val Loss: 1.3590\n",
      "Epoch: 9/10... Step: 1340... Loss: 1.3608... Val Loss: 1.3606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 1350... Loss: 1.3666... Val Loss: 1.3554\n",
      "Epoch: 9/10... Step: 1360... Loss: 1.3446... Val Loss: 1.3514\n",
      "Epoch: 9/10... Step: 1370... Loss: 1.3288... Val Loss: 1.3530\n",
      "Epoch: 9/10... Step: 1380... Loss: 1.3509... Val Loss: 1.3510\n",
      "Epoch: 9/10... Step: 1390... Loss: 1.3393... Val Loss: 1.3455\n",
      "Epoch: 10/10... Step: 1400... Loss: 1.2898... Val Loss: 1.3462\n",
      "Epoch: 10/10... Step: 1410... Loss: 1.2840... Val Loss: 1.3425\n",
      "Epoch: 10/10... Step: 1420... Loss: 1.3323... Val Loss: 1.3461\n",
      "Epoch: 10/10... Step: 1430... Loss: 1.3420... Val Loss: 1.3544\n",
      "Epoch: 10/10... Step: 1440... Loss: 1.3148... Val Loss: 1.3465\n",
      "Epoch: 10/10... Step: 1450... Loss: 1.3083... Val Loss: 1.3393\n",
      "Epoch: 10/10... Step: 1460... Loss: 1.3245... Val Loss: 1.3354\n",
      "Epoch: 10/10... Step: 1470... Loss: 1.3384... Val Loss: 1.3393\n",
      "Epoch: 10/10... Step: 1480... Loss: 1.3324... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 1490... Loss: 1.3082... Val Loss: 1.3310\n",
      "Epoch: 10/10... Step: 1500... Loss: 1.2954... Val Loss: 1.3331\n",
      "Epoch: 10/10... Step: 1510... Loss: 1.2801... Val Loss: 1.3248\n",
      "Epoch: 10/10... Step: 1520... Loss: 1.2874... Val Loss: 1.3268\n",
      "Epoch: 10/10... Step: 1530... Loss: 1.3038... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 1540... Loss: 1.3378... Val Loss: 1.3282\n",
      "Epoch: 10/10... Step: 1550... Loss: 1.3453... Val Loss: 1.3238\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded_text, epochs=10, n_seq=128, n_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (cv-nd)",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
