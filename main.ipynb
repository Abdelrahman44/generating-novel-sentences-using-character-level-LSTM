{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I construct novel sentences using LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/anna.txt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define encoding function to encode words to numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    chars = tuple(set(text))\n",
    "    int2char = dict(enumerate(chars))\n",
    "    char2int = {char: i for i, char in int2char.items()}\n",
    "    encoded = np.array([char2int[char] for char in text])  # [95, 6, 13, 95, ...] corresponding to text\n",
    "    return encoded, char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 72, 20,  0,  1, 59, 28, 58, 46, 66, 66, 66, 71, 20,  0,  0, 70,\n",
       "       58, 15, 20,  2, 54, 33, 54, 59, 26, 58, 20, 28, 59, 58, 20, 33, 33,\n",
       "       58, 20, 33, 54, 64, 59,  8, 58, 59, 78, 59, 28, 70, 58, 36, 40])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded, char2int = encode(text)\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):    #n_labels is the number of unique words (vocabulary), arr is the text\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    print(\"n_labels = {0},    first dim = {1}\".format(n_labels, np.multiply(*arr.shape)))\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.    #this creates the on-hot encoded vector\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting batches of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, n_seq, n_steps):\n",
    "    batch_size = n_seq * n_steps\n",
    "    n_batches = len(data) // batch_size\n",
    "    data = data[:n_batches*batch_size]          # drop some data to get only full batches\n",
    "    data = data.reshape((n_seq, -1))        \n",
    "    \n",
    "    for i in range(0, data.shape[1], n_steps):  # iterate on the columns to get the batches\n",
    "        x = data[:, i:i+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], data[:, i+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], data[:, 0]\n",
    "        yield x, y                              # x, y are generators that you can use next() on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[19 72 20  0  1 59 28 58 46 66]\n",
      " [58 20  2 58 40 48  1 58 76 48]\n",
      " [78 54 40 27 66 66 60 13 59 26]\n",
      " [40 58 17 36 28 54 40 76 58 72]\n",
      " [58 54  1 58 54 26 81 58 26 54]\n",
      " [58 51  1 58 41 20 26 66 48 40]\n",
      " [72 59 40 58 16 48  2 59 58 15]\n",
      " [ 8 58 35 36  1 58 40 48 41 58]\n",
      " [ 1 58 54 26 40 55  1 27 58  7]\n",
      " [58 26 20 54 17 58  1 48 58 72]]\n",
      "\n",
      "y\n",
      " [[72 20  0  1 59 28 58 46 66 66]\n",
      " [20  2 58 40 48  1 58 76 48 54]\n",
      " [54 40 27 66 66 60 13 59 26 81]\n",
      " [58 17 36 28 54 40 76 58 72 54]\n",
      " [54  1 58 54 26 81 58 26 54 28]\n",
      " [51  1 58 41 20 26 66 48 40 33]\n",
      " [59 40 58 16 48  2 59 58 15 48]\n",
      " [58 35 36  1 58 40 48 41 58 26]\n",
      " [58 54 26 40 55  1 27 58  7 72]\n",
      " [26 20 54 17 58  1 48 58 72 59]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, text, n_steps=100, n_hidden=256, n_layers=2, drop_prop=0.5, lr=0.001):\n",
    "        # n_steps: number of elements in each sequence in each bartch\n",
    "        # n_hidden: number of output elements of the intermediate layers\n",
    "        # n_layers: number of LSTM layers to use\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prop = drop_prop\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_steps = n_steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        slef.text = text\n",
    "        self.int2char = dict(enumerate(self.text))\n",
    "        self.char2int = {char: i for i, char in self.int2char}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.text), n_hidden, n_layers, dropout=drop_prop, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prop)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.text))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, x, h=None, cuda=False):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "            \n",
    "        x = np.array([[self.char2int[x]]])\n",
    "        x = one_hot_encode(x, len(self.char2int))\n",
    "        x = torch.from_numpy(x)\n",
    "        \n",
    "        if h == None:\n",
    "            h = init_hidden(1)\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            \n",
    "        out, h = self.forward(x, h)\n",
    "        out = F.softmax(out)\n",
    "        \n",
    "        out = out.numpy().squeeze()\n",
    "        \n",
    "        return self.int2char[]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python3 (cv-nd)",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
