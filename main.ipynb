{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I construct novel sentences using LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every un'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/anna.txt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define encoding function to encode words to numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    chars = tuple(set(text))\n",
    "    int2char = dict(enumerate(chars))\n",
    "    char2int = {char: i for i, char in int2char.items()}\n",
    "    encoded = np.array([char2int[char] for char in text])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 82,  0, 57, 12,  5, 20, 63, 79, 42, 42, 42, 67,  0, 57, 57, 47,\n",
       "       63,  2,  0, 24, 60, 81, 60,  5,  4, 63,  0, 20,  5, 63,  0, 81, 81,\n",
       "       63,  0, 81, 60, 55,  5, 74, 63,  5, 21,  5, 20, 47, 63, 27, 80])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode(text)\n",
    "encoded[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    print(\"n_labels = {0},    first dim = {1}\".format(n_labels, np.multiply(*arr.shape)))\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting batches of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, n_seq, n_steps):\n",
    "    batch_size = n_seq * n_steps\n",
    "    n_batches = len(data) // batch_size\n",
    "    data = data[:n_batches*batch_size]\n",
    "    data = data.reshape((n_seq, -1))\n",
    "    \n",
    "    for i in range(0, data.shape[1], n_steps):\n",
    "        x = data[:, i:i+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], data[:, i+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], data[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[19 82  0 57 12  5 20 63 79 42]\n",
      " [63  0 24 63 80 66 12 63 76 66]\n",
      " [21 60 80 75 42 42 11 28  5  4]\n",
      " [80 63 10 27 20 60 80 76 63 82]\n",
      " [63 60 12 63 60  4 39 63  4 60]\n",
      " [63 15 12 63 51  0  4 42 66 80]\n",
      " [82  5 80 63 26 66 24  5 63  2]\n",
      " [74 63  6 27 12 63 80 66 51 63]\n",
      " [12 63 60  4 80 62 12 75 63 68]\n",
      " [63  4  0 60 10 63 12 66 63 82]]\n",
      "\n",
      "y\n",
      " [[82  0 57 12  5 20 63 79 42 42]\n",
      " [ 0 24 63 80 66 12 63 76 66 60]\n",
      " [60 80 75 42 42 11 28  5  4 39]\n",
      " [63 10 27 20 60 80 76 63 82 60]\n",
      " [60 12 63 60  4 39 63  4 60 20]\n",
      " [15 12 63 51  0  4 42 66 80 81]\n",
      " [ 5 80 63 26 66 24  5 63  2 66]\n",
      " [63  6 27 12 63 80 66 51 63  4]\n",
      " [63 60  4 80 62 12 75 63 68 82]\n",
      " [ 4  0 60 10 63 12 66 63 82  5]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, data, n_steps=100, n_hidden=256, n_layers=2, drop_prop=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prop = drop_prop\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_steps = n_steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        slef.chars = data\n",
    "        self.char2int = dict(enumerate(self.chars))\n",
    "        self.int2char = {char: i for i, char in self.char2int}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers=n_layers, dropout=drop_prop, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prop)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    '''def predict(self, x, h=None, cuda=False):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else: \n",
    "            self.cpu()\n",
    "            \n",
    "        if h == None:\n",
    "            h = self.init_hidden(1)\n",
    "            \n",
    "        x = self.char2int[x]'''\n",
    "    \n",
    "    def init_weights(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python3 (cv-nd)",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
